# PaliGemma Fine-tuning Scripts

4ê°œì˜ VL ë°ì´í„°ì…‹ì— ëŒ€í•œ PaliGemma fine-tuning ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

## ğŸ¯ ì§€ì› ë°ì´í„°ì…‹

1. **COCO Captioning** - ì´ë¯¸ì§€ ìº¡ì…”ë‹
2. **DocVQA** - ë¬¸ì„œ ì´ë¯¸ì§€ ì§ˆì˜ì‘ë‹µ
3. **GQA** - ì‹œê°ì  ì¶”ë¡  ë° ì§ˆì˜ì‘ë‹µ
4. **VQAv2** - ì¼ë°˜ ì‹œê°ì  ì§ˆì˜ì‘ë‹µ

## ğŸ“‹ Requirements

```bash
pip install torch transformers datasets peft bitsandbytes accelerate pillow wandb
```

## ğŸš€ ì‚¬ìš©ë²•

### 1. COCO Captioning

```bash
python finetuning/coco_captioning.py \
  --model_path google/paligemma-3b-pt-448 \
  --output_dir ./models/paligemma-coco \
  --num_epochs 3 \
  --batch_size 4 \
  --learning_rate 2e-4 \
  --use_wandb
```

### 2. DocVQA

```bash
python finetuning/docvqa.py \
  --model_path google/paligemma-3b-pt-448 \
  --output_dir ./models/paligemma-docvqa \
  --num_epochs 5 \
  --batch_size 4 \
  --learning_rate 2e-4
```

### 3. GQA (Visual Reasoning)

```bash
python finetuning/gqa.py \
  --model_path google/paligemma-3b-pt-448 \
  --output_dir ./models/paligemma-gqa \
  --num_epochs 3 \
  --batch_size 4 \
  --learning_rate 2e-4
```

### 4. VQAv2

```bash
python finetuning/vqav2.py \
  --model_path google/paligemma-3b-pt-448 \
  --output_dir ./models/paligemma-vqav2 \
  --num_epochs 3 \
  --batch_size 4 \
  --learning_rate 2e-4
```

## âš™ï¸ ì£¼ìš” ê¸°ëŠ¥

### ğŸ”§ 4-bit Quantization
- **BitsAndBytes** NF4 quantization ì‚¬ìš©
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 75% ê°ì†Œ
- í•™ìŠµ ì†ë„ ìœ ì§€

### ğŸ’¾ Gradient Checkpointing
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í•™ìŠµ
- í° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°€ëŠ¥

### ğŸ¯ LoRA (Low-Rank Adaptation)
- íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  fine-tuning
- Rank: 16, Alpha: 32
- Target modules: attention & FFN layers

## ğŸ“Š ê¶Œì¥ í•˜ì´í¼íŒŒë¼ë¯¸í„°

| Dataset | Epochs | Batch Size | Learning Rate | Memory (GPU) |
|---------|--------|------------|---------------|--------------|
| COCO Caption | 3 | 4 | 2e-4 | ~18GB |
| DocVQA | 5 | 4 | 2e-4 | ~18GB |
| GQA | 3 | 4 | 2e-4 | ~18GB |
| VQAv2 | 3 | 4 | 2e-4 | ~18GB |

## ğŸ”„ í…ŒìŠ¤íŠ¸ìš© ì†Œê·œëª¨ í•™ìŠµ

ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìƒ˜í”Œ ìˆ˜ë¥¼ ì œí•œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
python finetuning/vqav2.py \
  --model_path google/paligemma-3b-pt-448 \
  --output_dir ./models/test \
  --num_epochs 1 \
  --batch_size 2 \
  --max_train_samples 1000 \
  --max_eval_samples 100
```

## ğŸ“ˆ Monitoring with W&B

```bash
# ë¡œê·¸ì¸ (ìµœì´ˆ 1íšŒ)
wandb login

# í•™ìŠµ ì‹œ --use_wandb í”Œë˜ê·¸ ì¶”ê°€
python finetuning/coco_captioning.py \
  --model_path google/paligemma-3b-pt-448 \
  --output_dir ./models/paligemma-coco \
  --use_wandb \
  --wandb_project my-paligemma-project
```

## ğŸ“ ëª¨ë¸ ë³‘í•© (Merging)

í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ë“¤ì„ ë³‘í•©í•˜ë ¤ë©´:

```bash
python merge_vlm.py \
  --base_model google/paligemma-3b-pt-448 \
  --model_a ./models/paligemma-vqav2 \
  --model_b ./models/paligemma-coco \
  --output ./models/merged-vqa-caption \
  --mode ties \
  --density 0.3
```

## ğŸ’¡ Tips

1. **ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ**: batch_size ì¤„ì´ê¸° ë˜ëŠ” gradient_accumulation_steps ëŠ˜ë¦¬ê¸°
2. **ë¹ ë¥¸ ìˆ˜ë ´**: learning_rateë¥¼ 3e-4ë¡œ ë†’ì´ê¸° (ë‹¨, overfitting ì£¼ì˜)
3. **ê¸´ í…ìŠ¤íŠ¸**: DocVQAëŠ” max_length=256 ì‚¬ìš© (ë¬¸ì„œ ì´í•´)
4. **ë°ì´í„°ì…‹ í¬ê¸°**: ì „ì²´ í•™ìŠµì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼ (VQAv2 ~400k samples)

## ï¿½ Evaluation Metrics

ê° íƒœìŠ¤í¬ë³„ë¡œ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ í‰ê°€ ì§€í‘œ:

### VQAv2 (ì¼ë°˜ VQA)
- **Metric**: VQA Accuracy
- **ê³„ì‚°ë²•**: `min(ë‹µë³€ ì¼ì¹˜ ìˆ˜ / 3, 1.0)`
- 10ëª…ì˜ annotator ì¤‘ ìµœì†Œ 3ëª…ì´ ë™ì¼í•œ ë‹µë³€ì„ í•œ ê²½ìš° ì •ë‹µ

```python
# ê°„ë‹¨í•œ êµ¬í˜„ ì˜ˆì‹œ
def vqa_accuracy(pred, gt_answers):
    """gt_answers: list of 10 human annotations"""
    count = sum(1 for ans in gt_answers if ans == pred)
    return min(count / 3.0, 1.0)
```

### DocVQA (ë¬¸ì„œ VQA)
- **Metric**: ANLS (Average Normalized Levenshtein Similarity)
- **ë²”ìœ„**: 0~1 (1ì´ ì™„ë²½í•œ ì¼ì¹˜)
- ë¬¸ì„œì—ì„œëŠ” ì •í™•í•œ ë§¤ì¹­ë³´ë‹¤ ìœ ì‚¬ë„ê°€ ì¤‘ìš”

```python
from Levenshtein import distance

def anls(pred, gt):
    """Average Normalized Levenshtein Similarity"""
    if len(gt) == 0:
        return 1.0 if len(pred) == 0 else 0.0
    edit_dist = distance(pred.lower(), gt.lower())
    max_len = max(len(pred), len(gt))
    return 1.0 - (edit_dist / max_len)
```

### GQA (Visual Reasoning)
- **Metric**: Accuracy + Consistency Score
- **Accuracy**: ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë‹µë³€ì˜ ë¹„ìœ¨
- **Consistency**: Compositional reasoning í‰ê°€

```python
def gqa_accuracy(pred, gt):
    """Simple exact match"""
    return 1.0 if pred.lower().strip() == gt.lower().strip() else 0.0
```

### COCO Captioning
- **ì£¼ìš” Metric**: CIDEr (Consensus-based Image Description Evaluation)
- **ë³´ì¡° Metrics**: BLEU-4, METEOR, ROUGE-L, SPICE

```python
# pycocoevalcap ì‚¬ìš©
from pycocoevalcap.cider.cider import Cider

cider = Cider()
score, scores = cider.compute_score(gts, res)
# gts: {image_id: [ref1, ref2, ...]}
# res: {image_id: [pred]}
```

## ğŸ§ª Evaluation ì‹¤í–‰

í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë³„ë„ë¡œ ì œê³µë©ë‹ˆë‹¤:

```bash
# VQAv2 í‰ê°€
python eval.py \
  --task vqav2 \
  --model_path ./models/paligemma-vqav2 \
  --data_root D:/VQA/cocoqa

# COCO Captioning í‰ê°€
python eval.py \
  --task captioning \
  --model_path ./models/paligemma-coco \
  --data_root D:/coco2017
```

**í•„ìš” íŒ¨í‚¤ì§€**:
```bash
pip install python-Levenshtein
pip install pycocoevalcap  # COCO captioning metrics
```

## ï¿½ğŸ› Troubleshooting

### CUDA Out of Memory
```bash
# batch_size ì¤„ì´ê¸°
--batch_size 2

# ë˜ëŠ” ë” aggressiveí•œ gradient accumulation
--gradient_accumulation_steps 8
```

### Dataset Loading Error
- ì¼ë¶€ ë°ì´í„°ì…‹ì€ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í•„ìš”
- HuggingFace ê³„ì • ë¡œê·¸ì¸ í™•ì¸: `huggingface-cli login`

### BitsAndBytes Error
```bash
# CUDA ë²„ì „ í™•ì¸
pip install bitsandbytes --upgrade
```
